{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monjjjjj/orchid_classification/blob/main/orchid_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnT0c1rxTfH1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python import keras\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 \n",
        "from tensorflow import keras\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import Sequential\n",
        "from keras import initializers, optimizers\n",
        "from keras.layers import Dense, Flatten, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from keras.applications.resnet import ResNet50, ResNet152\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmhKEp0OZqNU"
      },
      "outputs": [],
      "source": [
        "# google drive mounted\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw-qgCOEZ22M"
      },
      "outputs": [],
      "source": [
        "# data path\n",
        "train_dir = \"/content/drive/MyDrive/orchid_classification/training/\"\n",
        "label_dir = \"/content/drive/MyDrive/orchid_classification/label/label.csv\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnWhe1PnQFkM"
      },
      "outputs": [],
      "source": [
        "# read data\n",
        "def load_data():\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "  df = pd.read_csv(label_dir, index_col = \"filename\")\n",
        "  for p in os.listdir(train_dir):\n",
        "    if p != \"label.csv\":\n",
        "      path = train_dir + p\n",
        "      image = cv2.imread(path)\n",
        "      image = image.astype(\"float\") / 255.0\n",
        "      image = cv2.resize(image, IMG_SIZE, interpolation = cv2.INTER_AREA)\n",
        "      x_train.append(np.array(image))\n",
        "      y_train.append(df.loc[str(p),\"category\"])\n",
        "  return x_train,y_train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weNI7FkX8n78"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "x_train, y_train = load_data()\n",
        "\n",
        "# one-hot encoding\n",
        "x_train = np.array(x_train)\n",
        "num_classes = 219\n",
        "y_train = np_utils.to_categorical(y_train, num_classes, dtype = 'float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yARZ5qzsdv4E"
      },
      "outputs": [],
      "source": [
        "# debug\n",
        "#print(x_train[1:5])\n",
        "#print(y_train[1:5])\n",
        "#print(x_train.shape)\n",
        "#print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spilt data\n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(x_train, y_train, train_size=0.5, test_size=0.5, random_state=0, stratify = y_train)"
      ],
      "metadata": {
        "id": "tDx8SCtjSiGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rImh8Bih2Y4t"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "data_augmentation(x_train)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3uD7ixtF-Fl"
      },
      "outputs": [],
      "source": [
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center = False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center = False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization = False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization = False,  # divide each input by its std\n",
        "        zca_whitening = False,  # apply ZCA whitening\n",
        "        rotation_range = 0.5,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.2, # Randomly zoom image \n",
        "        width_shift_range = 0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range = 0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip = True)  # randomly flip images\n",
        "datagen.fit(x_train1)\n",
        "#datagen.fit(x_test1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu1iCGMMgxrY"
      },
      "outputs": [],
      "source": [
        "# build model for ResNet50\n",
        "def build_model():\n",
        "  model = Sequential()\n",
        "  model.add(ResNet50(include_top=False, weights='imagenet', input_tensor=None, pooling='avg', classes=219, input_shape=(224, 224, 3)))\n",
        "  #model.add(Flatten())\n",
        "  model.add(Dense(219,activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  keras.utils.plot_model(model, show_shapes=True, dpi=64, to_file='model2.png')\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training for ResNet50\n",
        "model = build_model()\n",
        "\n",
        "#train_history = model.fit(x=x_train, y=y_train, batch_size= BATCH_SIZE, epochs=20, verbose=1)\n",
        "train_history = model.fit_generator(datagen.flow(x_train1, y_train1, batch_size = BATCH_SIZE), validation_data=(x_test1, y_test1), epochs = 50, verbose = 1)\n"
      ],
      "metadata": {
        "id": "qxZBQjPglCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build model for ResNet152\n",
        "def build_model1():\n",
        "  model = Sequential()\n",
        "  model.add(ResNet152(include_top=False, weights='imagenet', input_tensor=None, pooling='avg', classes=219, input_shape=(224, 224, 3)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(219,activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  keras.utils.plot_model(model, show_shapes=True, dpi=64, to_file='model2.png')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "5PIiPNl_KMq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training for ResNet152\n",
        "model1 = build_model1()\n",
        "\n",
        "train_history1 = model1.fit(x=x_train, y=y_train, batch_size= BATCH_SIZE, epochs=20, verbose=1)\n"
      ],
      "metadata": {
        "id": "4cZqCfAdiv2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build model for VGG16\n",
        "def build_model2():\n",
        "  model = Sequential()\n",
        "  model.add(VGG16(include_top= False, weights='imagenet', input_tensor=None, pooling='avg', classes=219, input_shape=(224, 224, 3)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(219,activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  keras.utils.plot_model(model, show_shapes=True, dpi=64, to_file='model2.png')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "64tdGVywUu3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training for VGG16\n",
        "model2 = build_model2()\n",
        "\n",
        "train_history2 = model2.fit(x=x_train, y=y_train, batch_size= BATCH_SIZE, epochs=30, verbose=1)"
      ],
      "metadata": {
        "id": "8fAq_oTFV3q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6c7FkfP_UBo"
      },
      "outputs": [],
      "source": [
        "print(train_history.history.keys())\n",
        "#print(train_history1.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-BAuDou_Xfg"
      },
      "outputs": [],
      "source": [
        "def show_train_history(train_history,train,validation):\n",
        "  \n",
        "  if train == 'accuracy':\n",
        "    plt.plot(train_history.history[train])\n",
        "    plt.plot(train_history.history[validation])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "  else:\n",
        "    plt.plot(train_history.history[train])\n",
        "    plt.plot(train_history.history[validation])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "\n",
        "  plt.legend(['train','validation'],loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "show_train_history(train_history,'accuracy','val_accuracy')\n",
        "show_train_history(train_history,'loss','val_loss')\n",
        "#show_train_history(train_history1,'accuracy','val_accuracy')\n",
        "#show_train_history(train_history1,'loss','val_loss')\n",
        "#show_train_history(train_history2,'accuracy','val_accuracy')\n",
        "#show_train_history(train_history2,'loss','val_loss')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C00pPISM_ayG"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "#x = np.arange(20)\n",
        "plt.plot(train_history.history['accuracy'], label=ResNet50)\n",
        "#plt.plot(x, train_history1.history['accuracy'], label=ResNet152)\n",
        "#plt.plot(train_history2.history['accuracy'], label=VGG16)\n",
        "\n",
        "plt.plot(train_history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='lower right') \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for loss\n",
        "# x=np.arange(20)\n",
        "plt.plot(train_history.history['loss'])\n",
        "#plt.plot(train_history1.history['loss'])\n",
        "#plt.plot(train_history2.history['loss'])\n",
        "\n",
        "plt.plot(train_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right') \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kBTYvHwMTK8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "scores=model.evaluate(x_test1,y_test1)\n",
        "print()\n",
        "print('accuracy:',scores[1])"
      ],
      "metadata": {
        "id": "LHXdaIf4eZg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "mount_file_id": "1OCoUeo3SSFidOmgBCP4mfcZzmTe1oFA4",
      "authorship_tag": "ABX9TyMbkJ/OQHC8r9ihLjfblg6a",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}