{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1OCoUeo3SSFidOmgBCP4mfcZzmTe1oFA4",
      "authorship_tag": "ABX9TyOGbbCvVgdA5r0O47fzBkXZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monjjjjj/orchid_classification/blob/main/orchid_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnT0c1rxTfH1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python import keras\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 \n",
        "from tensorflow import keras\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from keras import Sequential\n",
        "from keras import initializers, optimizers\n",
        "from keras.layers import Dense, Flatten, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.applications.resnet import ResNet50\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# google drive mounted\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmhKEp0OZqNU",
        "outputId": "32f76db4-580f-4227-8d3d-cabe39a22dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "labels = []\n",
        "images = []\n",
        "paths = []\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/orchid_classification/training/\"\n",
        "label_dir = \"/content/drive/MyDrive/orchid_classification/label/label.csv\"\n",
        "\n",
        "#BATCH_SIZE = 32\n",
        "#IMG_SIZE = (224, 224)\n",
        "\n",
        "#train_dataset = image_dataset_from_directory(train_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
        "\n"
      ],
      "metadata": {
        "id": "rw-qgCOEZ22M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "for num in range(len(df)):\n",
        "    paths.append(os.path.join('train_dir', df['filename'][:len(df)][num]))\n",
        "    labels.append(df['category'][:len(df)][num])\n",
        "\n",
        "#print(len(paths))\n",
        "#print(paths[0:5])\n",
        "'''"
      ],
      "metadata": {
        "id": "LzcdaFvLkL8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "54114a22-8c3d-4bf1-ab7a-17fee79a972a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor num in range(len(df)):\\n    paths.append(os.path.join('train_dir', df['filename'][:len(df)][num]))\\n    labels.append(df['category'][:len(df)][num])\\n\\n#print(len(paths))\\n#print(paths[0:5])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read data by yi han\n",
        "def load_data():\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "  df = pd.read_csv(label_dir, index_col = \"filename\")\n",
        "  for p in os.listdir(train_dir):\n",
        "    if p != \"label.csv\":\n",
        "      path = train_dir + p\n",
        "      image = cv2.imread(path)\n",
        "      image = image.astype(\"float\") / 255.0\n",
        "      image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
        "      x_train.append(np.array(image))\n",
        "      y_train.append(df.loc[str(p),\"category\"])\n",
        "  return x_train,y_train\n",
        "\n"
      ],
      "metadata": {
        "id": "vnWhe1PnQFkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = load_data()"
      ],
      "metadata": {
        "id": "weNI7FkX8n78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# debug\n",
        "#print(x_train[1:5])\n",
        "#print(y_train[1:5])\n",
        "#print(x_train[1].shape)\n",
        "#print(y_train[1].shape)"
      ],
      "metadata": {
        "id": "yARZ5qzsdv4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# autotune\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "'''"
      ],
      "metadata": {
        "id": "O9GhAEGS2Rt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check all the files are images\n",
        "'''\n",
        "def start(Path):\n",
        "    filelist = os.listdir(Path)\n",
        "    for file in filelist:\n",
        "        print(file)\n",
        "        img = Image.open(Path+ file).convert('RGB')\n",
        "    #     # print(img)\n",
        "        img.save(Path + file)\n",
        "    print('Done!')\n",
        "if __name__ == '__main__':\n",
        "    start('/content/drive/MyDrive/orchid_classification/training/')\n",
        "'''"
      ],
      "metadata": {
        "id": "9uqaRW7WNPpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "'''\n",
        "def read_image(paths):\n",
        "  for path in paths:\n",
        "    image = cv2.imread(path, cv2.COLOR_GRAY2BGR)\n",
        "    image = image.astype(\"float\") / 255.0\n",
        "    image = cv2.resize(image,(224,224))\n",
        "    images.append(image)\n",
        "\n",
        "read_image('/content/drive/MyDrive/orchid_classification/training/')\n",
        "images = np.asarray(images)\n",
        "images = images.reshape((-1, 224, 224, 3))\n",
        "labels = to_categorical(labels, 219)\n",
        "print(1, labels)\n",
        "'''"
      ],
      "metadata": {
        "id": "5lIy3cRi5eF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])\n",
        "'''"
      ],
      "metadata": {
        "id": "rImh8Bih2Y4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.2, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n"
      ],
      "metadata": {
        "id": "l3uD7ixtF-Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build model\n",
        "def build_model():\n",
        "  model = Sequential()\n",
        "  model.add(ResNet50(include_top=False, weights='imagenet', input_tensor=None, pooling='avg', input_shape=(224, 224, 3)))\n",
        "  model.add(Flatten())\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  keras.utils.plot_model(model, show_shapes=True, dpi=64, to_file='model2.png')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "zu1iCGMMgxrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "model = build_model()\n",
        "\n",
        "train_history = model.fit(x=x_train, y=y_train, batch_size=32, epochs=20, verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "a3Nfqw8CmolJ",
        "outputId": "4d0fe497-0c48-4407-e6bf-e45d80d2be80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 2048)              23587712  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 2048)             8192      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,595,904\n",
            "Trainable params: 23,538,688\n",
            "Non-trainable params: 57,216\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1d7bf4c83f7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 987\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    988\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m     raise RuntimeError(\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"})"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_history.history.keys())"
      ],
      "metadata": {
        "id": "l6c7FkfP_UBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_train_history(train_history,train,validation):\n",
        "  \n",
        "  if train == 'accuracy':\n",
        "    plt.plot(train_history.history[train])\n",
        "    plt.plot(train_history.history[validation])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "  else:\n",
        "    plt.plot(train_history.history[train])\n",
        "    plt.plot(train_history.history[validation])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "\n",
        "  plt.legend(['train','validation'],loc='upper left')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "P-BAuDou_Xfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_train_history(train_history,'accuracy','val_accuracy')\n",
        "show_train_history(train_history,'loss','val_loss')"
      ],
      "metadata": {
        "id": "PgRa3kkv_aGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(train_history.history['accuracy'])\n",
        "plt.plot(train_history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left') \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C00pPISM_ayG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}